---
title: "KeyEmotions"
excerpt: "Autoregressive transformer model for emotion-conditioned symbolic music generation. <br/><img src='/images/KeyEmotions2.png'>"
collection: portfolio
---

<p>
    Generative music, created through autonomous algorithms, has become a transformative tool for composers. This Master's Thesis explores a deep learning model that generates symbolic music dynamically adapted to Russell's emotional quadrants.
</p>

<div style="display: flex; align-items: center; margin: 20px 0;">
    <div style="flex: 1;">
      <img src="/key_emotions/Circumplex_Russell_Model.png" alt="Russell's Circumplex Model" style="max-width: 100%; border: 1px solid #eee; border-radius: 8px;">
    </div>
    <div style="flex: 1; padding-left: 20px;">
      <p style="font-size: 0.95em; color: #555;">
        Emotion Framework: Russell's model categorizes emotions along two axes:<br>
        • Valence (positive ↔ negative)<br>
        • Arousal (high energy ↔ low energy).<br>
        The quadrants define the emotional palette for the generated music.
      </p>
    </div>
 </div>

<p>
    Through a machine learning pipeline, I curated a labeled MIDI dataset, extracted emotion-relevant features (tempo, harmony, etc.), and designed a transformer model evaluated via both quantitative metrics and listener studies. The work also confronts ethical and legal questions: <em>Can AI be truly creative? Who owns its outputs?</em>  
</p>

<div style="margin: 1.5rem 0;">
    <a href="/key_emotions/Memoria_Latex_TFM_VIU_09MIAR.pdf" target="_blank" class="btn btn--research">View Full Thesis in Spanish (PDF)</a>
    <span style="display: inline-block; margin-left: 1rem; font-size: 0.9em; color: #666;">Includes complete technical details and ethical analysis</span>
</div>

<h1>
    Demo
</h1>

<img src="/key_emotions/KeyEmotions_GUI_Demo.gif" alt="KeyEmotions GIF" style="max-width:100%; height:auto; display:block; margin: 20px auto;">

<script src="https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.4.0"></script>

<h2> Generated midi</h2>

<h3>Excited (High Arousal, Positive Valence)</h3>
<midi-visualizer 
    type="piano-roll" 
    id="visualizer1" 
    src="/key_emotions/1_generation.mid">
</midi-visualizer>
<midi-player 
    src="/key_emotions/1_generation.mid" 
    sound-font 
    visualizer="#visualizer1">
</midi-player>

<h3>Anger (High Arousal, Negative Valence)</h3>
<midi-visualizer 
    type="piano-roll" 
    id="visualizer2" 
    src="/key_emotions/2_generation.mid">
</midi-visualizer>
<midi-player 
    src="/key_emotions/2_generation.mid" 
    sound-font 
    visualizer="#visualizer2">
</midi-player>

<h3>Sadness (Low Arousal, Negative Valence)</h3>
<midi-visualizer 
    type="piano-roll" 
    id="visualizer3" 
    src="/key_emotions/3_generation.mid">
</midi-visualizer>
<midi-player 
    src="/key_emotions/3_generation.mid" 
    sound-font 
    visualizer="#visualizer3">
</midi-player>

<h3>Calm (Low Arousal, Positive Valence)</h3>
<midi-visualizer 
    type="piano-roll" 
    id="visualizer4" 
    src="/key_emotions/4_generation.mid">
</midi-visualizer>
<midi-player 
    src="/key_emotions/4_generation.mid" 
    sound-font 
    visualizer="#visualizer4">
</midi-player>




